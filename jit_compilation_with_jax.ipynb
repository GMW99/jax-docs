{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jit with Jax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Jax transforms work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To transform python fuctions, Jax converts the function into an intermediate lanauge called jaxpr. The Jax transformations then work on the jaxpr representation of the function.\n",
    "\n",
    "Jax does not deal with side effects and is functional in this manner so in the code below we would expect the jaxpr to ignore the global_list.append(x) as this is a side effect within the code, as it modifies some state variable value(s) outside its local environment, that is to say has an observable effect besides returning a value. In this case the modification of global_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ lambda ; a:f32[]. let\n",
      "    b:f32[] = log a\n",
      "    c:f32[] = log 2.0\n",
      "    d:f32[] = div b c\n",
      "  in (d,) }\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "global_list = []\n",
    "\n",
    "def log2(x):\n",
    "  global_list.append(x)\n",
    "  ln_x = jnp.log(x)\n",
    "  ln_2 = jnp.log(2.0)\n",
    "  return ln_x / ln_2\n",
    "\n",
    "print(jax.make_jaxpr(log2)(3.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is purely functional representation of the function provided to the jaxpr, ignoring the side effect.\n",
    "\n",
    "Its important to note that global_list is interacted on in the first pass by the Jax tracer object, which is used to construct the entire function. However, the tracers do not record the side-effects so they do not appear in the jaxpr but they do happen in the trace itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One should not rely on this as its strictly an implemtation detail**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important thing to understand is that jaxpr jaxpr capture the function as executed on the parameters given. Therefore if there is a condtional jaxpr will only construct the lanague on the branch taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ lambda ; a:i32[3]. let  in (a,) }\n"
     ]
    }
   ],
   "source": [
    "def log2_if_rank_2(x):\n",
    "  if x.ndim == 2:\n",
    "    ln_x = jnp.log(x)\n",
    "    ln_2 = jnp.log(2.0)\n",
    "    return ln_x / ln_2\n",
    "  else:\n",
    "    return x\n",
    "\n",
    "print(jax.make_jaxpr(log2_if_rank_2)(jax.numpy.array([1, 2, 3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ lambda ; a:f32[2,2]. let\n",
      "    b:f32[2,2] = log a\n",
      "    c:f32[] = log 2.0\n",
      "    d:f32[] = convert_element_type[new_dtype=float32 weak_type=False] c\n",
      "    e:f32[2,2] = div b d\n",
      "  in (e,) }\n"
     ]
    }
   ],
   "source": [
    "print(jax.make_jaxpr(log2_if_rank_2)(jax.numpy.array([[1.0, 2.0],[1.0,2.0]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## JIT compiling a function\n",
    "\n",
    "Jax allows for code to be run on the CPU/GPU/TPU\n",
    "using exactly the same code.\n",
    "\n",
    "Here we are going to look at the SELU operation:\n",
    "\n",
    "$$ SELU(x) = \\lambda \\begin{cases}\n",
    "    \\mbox{$x$} & \\mbox{if } x > 0\\\\\n",
    "    \\mbox{$\\alpha e^x-\\alpha$} & \\mbox{if } x \\leq 0\n",
    "    \\end{cases} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "994 µs ± 88.9 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def selu(x, alpha=1.67, lambda_=1.05):\n",
    "    return lambda_ * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
    "\n",
    "x = jnp.arange(1000000)\n",
    "%timeit selu(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example the code is sending one operation at a time to the accelerator limimiting the XLA compilers ability to optimize the function.\n",
    "\n",
    "\n",
    "For this code to be the most performant, we want to give the XLA compiler as much code as possible. For this jax provides the jax.jit transformation, this will jit a jax-compatible function, to speed it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.1 µs ± 713 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "selu_jit = jax.jit(selu)\n",
    "\n",
    "# Warm up\n",
    "selu_jit(x).block_until_ready()\n",
    "\n",
    "%timeit selu_jit(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that is an order of mangnituded faster, so what have we done:\n",
    "\n",
    "1. We compiled selu with jax.jit(selu) and called it selu_jit\n",
    "2. We when ran selu_jit once on x, such that Jax can do the tracing. The jaxpr is then compiled using XLA into effiecent code. Now subsequent calls use the optimised compiled code.\n",
    "\n",
    "Meaning that we no longer use the old python implemention at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If we didn't include the warm-up call seperately, everything still works as expected, however the compilitation time would be included in the benchmark as well. It would still be faster but would not be a fair comparision.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why can we not JIT everything?\n",
    "\n",
    "After the above example, it may be tempting to think we can just jit everything. However, this is not the case and there are time when jitting is approtiate and times when it is not.\n",
    "\n",
    "The following example is a case where it doesn't work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Conditional functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConcretizationTypeError",
     "evalue": "Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(bool[], weak_type=True)>with<DynamicJaxprTrace(level=0/1)>\nThe problem arose with the `bool` function. \nWhile tracing the function f at /tmp/ipykernel_4822/1879943193.py:3 for jit, this concrete value was not available in Python because it depends on the value of the argument 'x'.\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConcretizationTypeError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4822/1879943193.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mf_jit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mf_jit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Should raise an error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "    \u001b[0;31m[... skipping hidden 13 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4822/1879943193.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/jax/core.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m    998\u001b[0m                       f\"or `jnp.array(x, {fun.__name__})` instead.\")\n\u001b[1;32m    999\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1000\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mConcretizationTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1001\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConcretizationTypeError\u001b[0m: Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(bool[], weak_type=True)>with<DynamicJaxprTrace(level=0/1)>\nThe problem arose with the `bool` function. \nWhile tracing the function f at /tmp/ipykernel_4822/1879943193.py:3 for jit, this concrete value was not available in Python because it depends on the value of the argument 'x'.\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError"
     ]
    }
   ],
   "source": [
    "# Condition on value of x.\n",
    "\n",
    "def f(x):\n",
    "  if x > 0:\n",
    "    return x\n",
    "  else:\n",
    "    return 2 * x\n",
    "\n",
    "f_jit = jax.jit(f)\n",
    "f_jit(10)  # Should raise an error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. While loop conditations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConcretizationTypeError",
     "evalue": "Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(bool[], weak_type=True)>with<DynamicJaxprTrace(level=0/1)>\nThe problem arose with the `bool` function. \nWhile tracing the function g at /tmp/ipykernel_4822/4172967325.py:1 for jit, this concrete value was not available in Python because it depends on the value of the argument 'n'.\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConcretizationTypeError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4822/4172967325.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mg_jit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mg_jit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Should raise an error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "    \u001b[0;31m[... skipping hidden 13 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4822/4172967325.py\u001b[0m in \u001b[0;36mg\u001b[0;34m(x, n)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/jax/core.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m    998\u001b[0m                       f\"or `jnp.array(x, {fun.__name__})` instead.\")\n\u001b[1;32m    999\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1000\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mConcretizationTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1001\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConcretizationTypeError\u001b[0m: Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(bool[], weak_type=True)>with<DynamicJaxprTrace(level=0/1)>\nThe problem arose with the `bool` function. \nWhile tracing the function g at /tmp/ipykernel_4822/4172967325.py:1 for jit, this concrete value was not available in Python because it depends on the value of the argument 'n'.\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError"
     ]
    }
   ],
   "source": [
    "def g(x, n):\n",
    "  i = 0\n",
    "  while i < n:\n",
    "    i += 1\n",
    "  return x + i\n",
    "\n",
    "g_jit = jax.jit(g)\n",
    "g_jit(10, 20)  # Should raise an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem here, is that we are trying to jit on a value that is being conditioned. It should be clear as to why this is not possible as the jaxpr depends on the actual values to trace it. See how jit transforms work.\n",
    "\n",
    "The more specific information about the values we use in the trace, the more the standard Python control flow can be used. However, at the cost of being too specific means that we can not resuse the same traced function values. Jax deals with this by tracing at different levels of abstractions.\n",
    "\n",
    "For `jax.jit`, the default level is `ShapedArray` - that is, each tracer has a concrete shape, this we can condition on), but it has no concrete value. What this allows, is that compiled version can work on all possible inputs as long as they have the same shape. This means that if we try and condition on a value we get an error as the tracer has no concrete value.\n",
    "\n",
    "In `jax.grad`, the constraints are more relaxed, so you can do more. If you compose several transformations, however, you must satisfy the constraints of the most strict one. So, if you `jit(grad(f))`, `f` must not condition on value. The docs go into more detail about the sharp bits of Jax controll flow.\n",
    "\n",
    "\n",
    "A nice table outlines when to use `jit` or `grad` \n",
    "\n",
    "| Construct      | jit    | grad |\n",
    "|----------------|--------|------|\n",
    "| if             | **NO** | YES  |\n",
    "| for            | YES*   | YES  |\n",
    "| while          | YES*   | YES  |\n",
    "| lax.cond       | YES    | YES  |\n",
    "| lax.while_loop | YES    | fwd  |\n",
    "| lax.fori_loop  | YES    | fwd  |\n",
    "| lax.scan       | YES    | YES  |\n",
    "\n",
    "∗ = argument-value-independent loop condition - unrolls the loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So one way to deal with the problem is to rewrite the code to avoid conditionals on value. Another option is to use special jax control flow operations like `jax.lax.cond`. This is sometimes impossible to do. In situations like this you can consider jitting only part of the function. For example the more computationaly expensive part inside the loop. i.e. **MAKE SURE YOU CHECK CACHING TO AVOID SHOOTING YOURSELF IN THE FOOT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(30, dtype=int32, weak_type=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# While loop conditioned on x and n with a jitted body.\n",
    "\n",
    "@jax.jit\n",
    "def loop_body(prev_i):\n",
    "  return prev_i + 1\n",
    "\n",
    "def g_inner_jitted(x, n):\n",
    "  i = 0\n",
    "  while i < n:\n",
    "    i = loop_body(i)\n",
    "  return x + i\n",
    "\n",
    "g_inner_jitted(10, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In siutations where we really need to JIT a function that has a condition on the value of an input. We can tell Jax to use a less abstract tracer for a specific input, by using `static_argnums`. The cost of this is that the resulting jaxpr is less a lot flexible. Which means jax will have to recompile the function with every new value of the specific input. It is a good strat if the function is guaranteed to get a limited set of different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "f_jit_correct = jax.jit(f, static_argnums=0)\n",
    "print(f_jit_correct(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "g_jit_correct = jax.jit(g, static_argnums=1)\n",
    "print(g_jit_correct(10, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to use Jit\n",
    "\n",
    "In the example above, jitting is not actually worth it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g jitted:\n"
     ]
    }
   ],
   "source": [
    "print(\"g jitted:\")\n",
    "%timeit g_jit_correct(10, 20).block_until_ready()\n",
    "\n",
    "print(\"g:\")\n",
    "%timeit g(10, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because `jax.jit` introduces some overhead as well. Therefore it usually only saves time if the complied function is complex and the plan is to run it many times. This is a common usecase in Machine learning though :-) \n",
    "\n",
    "**Generally, you want to jit the largest possible chunk of computation, i.e. the entire update step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
